{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae402f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Written by Eric-Finley Robins\n",
    "# Q-Learning algorithm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    " \n",
    "N_games = 10000 \n",
    "size_board = 4\n",
    "env = Chess_Env(size_board)\n",
    "S ,X, allowed_a = env.Initialise_game()\n",
    "np.random.seed(3420)\n",
    "\n",
    "input_neurons = np.shape(X)[0]    \n",
    "hidden_neurons = 200                \n",
    "output_neurons = np.shape(allowed_a)[0]   \n",
    "\n",
    "w1 = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(1/input_neurons)\n",
    "w2 = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(1/hidden_neurons)\n",
    "b1 = np.zeros((hidden_neurons,))\n",
    "b2 = np.zeros((output_neurons,))\n",
    " \n",
    "# Hyperparameters\n",
    "epsilon = 0.15     \n",
    "decay = 0.99985     \n",
    "discount = 0.85        \n",
    "learning_rate = 0.0035        \n",
    "\n",
    "# Finds indices of allowed actions \n",
    "# Generates random integer\n",
    "# Perfoms either greedy or random selection from allowed integers\n",
    "def EpsilonGreedy_Policy(output_out, epsilon, allowed_a):\n",
    "    \n",
    "    allowed_i = np.where(allowed_a == 1)[0]\n",
    "\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        a_agent = allowed_i[np.random.randint(len(allowed_i))]\n",
    "    else:\n",
    "        output_values = output_out[allowed_i]\n",
    "        a_agent = allowed_i[np.argmax(output_values)]\n",
    " \n",
    "    return a_agent  \n",
    "\n",
    "wins = 0\n",
    "wins_ = []\n",
    "\n",
    "move_total = 0\n",
    "moves_ = []\n",
    "\n",
    "for games in range(N_games + 1):\n",
    " \n",
    "    epsilon *= decay\n",
    "    Done = 0                                                            \n",
    "    moves = 0\n",
    "    S, X, allowed_a = env.Initialise_game()      \n",
    "\n",
    "    # For calculating ema's\n",
    "    if games % 100 == 0:\n",
    "        moves_.append(move_total/100)\n",
    "        move_total = 0\n",
    "        wins_.append(wins)\n",
    "        wins = 0\n",
    "\n",
    "    # Loop for each game\n",
    "    while Done == 0:                     \n",
    "        \n",
    "        # Forward Prop for action determination\n",
    "        hidden_in = np.dot(w1, X) + b1\n",
    "        hidden_out = (hidden_in > 0)* hidden_in\n",
    " \n",
    "        output_in = np.dot(w2, hidden_out) + b2\n",
    "        output = (output_in > 0) * output_in\n",
    "        output_masked = output * allowed_a.flatten()\n",
    " \n",
    "        agent = EpsilonGreedy_Policy(output, epsilon, allowed_a)\n",
    " \n",
    "        S_next, X_next, allowed_a_next, R, Done = env.OneStep(agent)       \n",
    "        moves += 1\n",
    "\n",
    "        if R == 1:\n",
    "            wins += 1\n",
    "\n",
    "        Q_current = output[agent]\n",
    " \n",
    "        filter = np.zeros((output_neurons, ))\n",
    "        filter[agent] = 1\n",
    "    \n",
    "        if Done==1:\n",
    "            \n",
    "            Error = R - Q_current # Temporal Error Calculation \n",
    "\n",
    "            # Backpropagation of error\n",
    "            reluOutput = np.where(output_in > 0, 1, 0)    \n",
    "            delta2 = reluOutput * Error * filter\n",
    " \n",
    "            dw2 = np.outer(delta2, hidden_out)\n",
    "            db2 = delta2\n",
    " \n",
    "            reluInput = np.where(hidden_in > 0, 1, 0)\n",
    "            delta1 = reluInput * np.dot(w2.T, delta2)\n",
    " \n",
    "            dw1 = np.outer(delta1, X)\n",
    "            db1 = delta1\n",
    "            move_total += moves\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Forward prop for future reward prediction\n",
    "            hidden_in_next = np.dot(w1, X_next) + b1\n",
    "            hidden_out_next = (hidden_in_next > 0) * hidden_in_next\n",
    "            output_in_next = np.dot(w2, hidden_out_next) + b2\n",
    "            output_next = (output_in_next > 0) * output_in_next\n",
    "            \n",
    "            Q_max = np.max(output_next * allowed_a_next.flatten())      # Maximum Q-value in the next state\n",
    "            Q_target = R + discount * Q_max                             # Q-target for the current state-action pair\n",
    "            Error = Q_target - Q_current                                # Temporal Error \n",
    " \n",
    "            # Backpropagation\n",
    "            reluOutput = np.where(output_in > 0, 1, 0)\n",
    "            delta2 = reluOutput * Error * filter\n",
    " \n",
    "            dw2 = np.outer(delta2, hidden_out)\n",
    "            db2 = delta2\n",
    " \n",
    "            # Backpropagation: hidden layer -> input layer\n",
    "            reluInput = np.where(hidden_in > 0, 1, 0)\n",
    "            delta1 = reluInput * np.dot(w2.T, delta2)\n",
    " \n",
    "            dw1 = np.outer(delta1, X)\n",
    "            db1 = delta1\n",
    "\n",
    "        # Update Rules\n",
    "        w1 += learning_rate * dw1\n",
    "        w2 += learning_rate * dw2\n",
    "        b1 += learning_rate * db1\n",
    "        b2 += learning_rate * db2            \n",
    "\n",
    "        # Passing forward from the step taken\n",
    "        allowed_a = allowed_a_next\n",
    "        S = S_next\n",
    "        X = X_next\n",
    "\n",
    "# X Axis labels\n",
    "x_ticks = np.linspace(0, N_games, num=int(N_games/100)+1)\n",
    "\n",
    "# Plotting EMA Graph for win %\n",
    "wins_ = wins_\n",
    "wins_series = pd.Series(wins_)\n",
    "wins_ema = wins_series.ewm(alpha=0.2).mean()\n",
    "\n",
    "plt.plot(x_ticks, wins_ema)\n",
    "plt.xlabel('Games Played')\n",
    "plt.xticks(x_ticks[::20])\n",
    "plt.ylabel('Reward Earned')\n",
    "plt.title('Q-learning Average Reward Earned')\n",
    "plt.show()\n",
    "\n",
    "# Plotting EMA Graph for avg. moves taken\n",
    "moves_ = moves_\n",
    "moves_series = pd.Series(moves_)\n",
    "moves_ema = moves_series.ewm(alpha=0.2).mean()\n",
    "\n",
    "plt.plot(x_ticks[1:], moves_ema[1:])\n",
    "plt.xlabel('Games Played')\n",
    "plt.xticks(x_ticks[::20])\n",
    "plt.ylabel('Moves Taken')\n",
    "plt.title('Q-Learning Average Moves Taken')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
